{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenize\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "project_path = \"M:\\\\DOCUMENTS\\\\JupyterLab\\\\Standard\\\\word_embeddings\"\n",
    "os.chdir(project_path)\n",
    "directory_path = os.getcwd() + \"\\\\textbook_corpora\\\\Guyton_and_Hall_textbook_of_Medical_Physiology\"\n",
    "tokenized_sentences_list = []\n",
    "\n",
    "os.chdir(directory_path)\n",
    "# print(directory_path)\n",
    "\n",
    "# Preparing dataset\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{directory_path}\\{file}\"\n",
    "        print(file_path)\n",
    "    with open(file_path, encoding=\"utf8\") as f:\n",
    "        file_contents = f.read()\n",
    "        print(file_contents)\n",
    "        file_contents = file_contents.replace('\\n', ' ')\n",
    "        file_contents_sentences = file_contents.split('.')\n",
    "    #     print(file_contents_sentences)\n",
    "        for sentence in file_contents_sentences:\n",
    "            tokenized_sentence = tokenize(sentence.lower())\n",
    "            tokenized_sentences_list.append(list(tokenized_sentence))\n",
    "            print(list(tokenized_sentence))\n",
    "        print(tokenized_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bert_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertEmbedding\n\u001b[0;32m      3\u001b[0m bert_embedding \u001b[38;5;241m=\u001b[39m BertEmbedding()\n\u001b[0;32m      4\u001b[0m bert_embedding(tokenized_sentences_list)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bert_embedding'"
     ]
    }
   ],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "bert_embedding = BertEmbedding()\n",
    "bert_embedding(tokenized_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14029, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Model configuration and summary\n",
    "\n",
    "model = Word2Vec(tokenized_sentences_list, vector_size=300, min_count=1, workers=8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "models_path = project_path + \"\\\\models\\\\\"\n",
    "os.chdir(models_path)\n",
    "os.getcwd()\n",
    "\n",
    "# words = list(model.wv.key_to_index)\n",
    "# print(model.wv.key_to_index)\n",
    "# print(model['sentence'])\n",
    "# model.save('model.bin')\n",
    "current_version = \"_0_1_4\"\n",
    "model.wv.save_word2vec_format(models_path + 'modelw2v_GandH_Medical_Physiology' + current_version + '.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Generate tsv output files\n",
    "\n",
    "os.chdir(project_path)\n",
    "ret = os.system(\"python -m gensim.scripts.word2vec2tensor -i models\\modelw2v_GandH_Medical_Physiology\" + current_version + \".bin -o tsv_output\\GandH_Medical_Physiology\" + current_version)\n",
    "\n",
    "if ret is 0:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = Word2Vec.load('model.bin')\n",
    "# print(new_model)\n",
    "\n",
    "# X = model[model.wv.key_to_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
